{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fdda2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from fugashi import Tagger\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb060d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kanji data\n",
    "kanji_df = pd.read_json(\"kanji.json\")\n",
    "tagger = Tagger('-Owakati')\n",
    "kanji_df = kanji_df.drop(['strokes', 'grade', 'freq', 'jlpt_old', 'jlpt_new', 'meanings', 'wk_radicals', 'wk_readings_kun', 'wk_readings_on', 'wk_meanings', 'wk_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0eba3713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一</th>\n",
       "      <th>二</th>\n",
       "      <th>九</th>\n",
       "      <th>七</th>\n",
       "      <th>人</th>\n",
       "      <th>入</th>\n",
       "      <th>八</th>\n",
       "      <th>力</th>\n",
       "      <th>十</th>\n",
       "      <th>三</th>\n",
       "      <th>...</th>\n",
       "      <th>視</th>\n",
       "      <th>謁</th>\n",
       "      <th>謹</th>\n",
       "      <th>賓</th>\n",
       "      <th>贈</th>\n",
       "      <th>辶</th>\n",
       "      <th>逸</th>\n",
       "      <th>難</th>\n",
       "      <th>響</th>\n",
       "      <th>頻</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>readings_on</th>\n",
       "      <td>[いち, いつ]</td>\n",
       "      <td>[に, じ]</td>\n",
       "      <td>[きゅう, く]</td>\n",
       "      <td>[しち]</td>\n",
       "      <td>[じん, にん]</td>\n",
       "      <td>[にゅう, じゅ]</td>\n",
       "      <td>[はち]</td>\n",
       "      <td>[りょく, りき, りい]</td>\n",
       "      <td>[じゅう, じっ, じゅっ]</td>\n",
       "      <td>[さん, ぞう]</td>\n",
       "      <td>...</td>\n",
       "      <td>[し]</td>\n",
       "      <td>[えつ]</td>\n",
       "      <td>[きん]</td>\n",
       "      <td>[ひん]</td>\n",
       "      <td>[そう, ぞう]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[なん]</td>\n",
       "      <td>[きょう]</td>\n",
       "      <td>[ひん]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readings_kun</th>\n",
       "      <td>[ひと-, ひと.つ]</td>\n",
       "      <td>[ふた, ふた.つ, ふたたび]</td>\n",
       "      <td>[ここの, ここの.つ]</td>\n",
       "      <td>[なな, なな.つ, なの]</td>\n",
       "      <td>[ひと, -り, -と]</td>\n",
       "      <td>[い.る, -い.る, -い.り, い.れる, -い.れ, はい.る]</td>\n",
       "      <td>[や, や.つ, やっ.つ, よう]</td>\n",
       "      <td>[ちから]</td>\n",
       "      <td>[とお, と]</td>\n",
       "      <td>[み, み.つ, みっ.つ]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[つつしむ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[おくる]</td>\n",
       "      <td>[しんにょう]</td>\n",
       "      <td>[しんにょう]</td>\n",
       "      <td>[かたい, むずかしい]</td>\n",
       "      <td>[ひびく]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 13108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        一                 二             九               七  \\\n",
       "readings_on      [いち, いつ]            [に, じ]      [きゅう, く]            [しち]   \n",
       "readings_kun  [ひと-, ひと.つ]  [ふた, ふた.つ, ふたたび]  [ここの, ここの.つ]  [なな, なな.つ, なの]   \n",
       "\n",
       "                         人                                    入  \\\n",
       "readings_on       [じん, にん]                            [にゅう, じゅ]   \n",
       "readings_kun  [ひと, -り, -と]  [い.る, -い.る, -い.り, い.れる, -い.れ, はい.る]   \n",
       "\n",
       "                               八              力               十  \\\n",
       "readings_on                 [はち]  [りょく, りき, りい]  [じゅう, じっ, じゅっ]   \n",
       "readings_kun  [や, や.つ, やっ.つ, よう]          [ちから]         [とお, と]   \n",
       "\n",
       "                           三  ...    視     謁       謹     賓         贈        辶  \\\n",
       "readings_on         [さん, ぞう]  ...  [し]  [えつ]    [きん]  [ひん]  [そう, ぞう]       []   \n",
       "readings_kun  [み, み.つ, みっ.つ]  ...   []    []  [つつしむ]    []     [おくる]  [しんにょう]   \n",
       "\n",
       "                    逸             難      響     頻  \n",
       "readings_on        []          [なん]  [きょう]  [ひん]  \n",
       "readings_kun  [しんにょう]  [かたい, むずかしい]  [ひびく]    []  \n",
       "\n",
       "[2 rows x 13108 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanji_df#['食']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "134728da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "きのう\n",
      "、\n",
      "りんご\n",
      "を\n",
      "たべ\n",
      "た\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "word_list = []\n",
    "pos_list = []\n",
    "text = \"きのう、りんごをたべた\"\n",
    "#text = 'たべた'\n",
    "#text = 'いち'\n",
    "tagger.parse(text)\n",
    "# => '麩 菓子 は 、 麩 を 主材 料 と し た 日本 の 菓子 。'\n",
    "for word in tagger(text):\n",
    "    output.append(word.feature.lemma)\n",
    "    word_list.append(word.char_type)\n",
    "    pos_list.append(word.pos)\n",
    "    print(word.feature.orth)\n",
    "    #print(word.feature.lemma, sep=' ')\n",
    "    # \"feature\" is the Unidic feature data as a named tuple\n",
    "#word_list = ' '.join(word_list)\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4a7758e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "たべ\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "normalize() argument 2 must be str, not fugashi.fugashi.UnidicNode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m readings_on \u001b[38;5;129;01min\u001b[39;00m kanji_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadings_on\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#print(readings_on)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m reading \u001b[38;5;129;01min\u001b[39;00m readings_on:\n\u001b[1;32m----> 6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43municodedata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNFC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreading\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;28mprint\u001b[39m(reading)\n",
      "\u001b[1;31mTypeError\u001b[0m: normalize() argument 2 must be str, not fugashi.fugashi.UnidicNode"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    print(word)\n",
    "    for readings_on in kanji_df.loc['readings_on']:\n",
    "        #print(readings_on)\n",
    "        for reading in readings_on:\n",
    "            if unicodedata.normalize('NFC', word) == u reading:\n",
    "                print(reading)\n",
    "                #print(wordreading)\n",
    "            #else:\n",
    "                #print(word, reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9625f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "いち\n"
     ]
    }
   ],
   "source": [
    "if 'いち' == 'いち':\n",
    "    print('いち')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58361eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0bb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text\n",
    "# use pos tagger\n",
    "# get new input\n",
    "# use hmm to access correctedness \n",
    "\n",
    "#WIll TO DO\n",
    "#What do you need to be successful?\n",
    "\n",
    "#text = input(\"input text: \")\n",
    "output = []\n",
    "word_list = []\n",
    "pos_list = []\n",
    "text = \"きのう、りんごをたべた\"\n",
    "#target = \"昨日、林檎を食べた\"\n",
    "#tokenize sentence(s) to words\n",
    "#The tagger things\n",
    "tagger.parse(text)\n",
    "for word in tagger(text):\n",
    "    #tokenize word to char\n",
    "    #n! amount of combinations \n",
    "    #print all combinations of chars\n",
    "        # (if) compare all combinations of char to on and kun readings\n",
    "            #ignore particles for now\n",
    "            #store kanji (and leftovers)\n",
    "            \n",
    "        #need to pair/add new dataset\n",
    "        #HMM (probability, haven't refreshed on project yet, but I think it should follow this) \n",
    "        #for combinations in new dataset (how often does it show up... たべ田 probably shows up less than 食べた)\n",
    "            #ideally check best option w/ prob of previous word existing before it (can use tags for HMM, word.pos)\n",
    "            #    #return best one \n",
    "#build our own test sentences?..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
