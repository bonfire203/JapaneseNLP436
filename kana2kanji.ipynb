{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdda2f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import unicodedata\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082cad4",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Converting test, train and val datasets which include kanji to be purely kana (furigana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a864398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (C:/Users/wlaw5/.cache/huggingface/datasets/universal_dependencies/ja_gsd/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15f397237af4d329c522a885ebbcb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dev-s1</td>\n",
       "      <td>ただし、50周年ソングに変更後は、EDも歌つきのものが使われた。</td>\n",
       "      <td>[ただし, 、, 50, 周年, ソング, に, 変更, 後, は, 、, ED, も, 歌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev-s2</td>\n",
       "      <td>私は初めてだったんだけど思っていたよりも魚は新鮮でした。</td>\n",
       "      <td>[私, は, 初めて, だっ, た, ん, だ, けど, 思っ, て, い, た, より, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev-s4</td>\n",
       "      <td>セントラル・リーグ審判員の水落朋大は実兄。</td>\n",
       "      <td>[セントラル, ・, リーグ, 審判, 員, の, 水落, 朋大, は, 実兄, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dev-s5</td>\n",
       "      <td>多彩なライブアクトとともに、祝日前の渋谷の夜を盛り上げてくれそうだ。</td>\n",
       "      <td>[多彩, な, ライブ, アクト, と, とも, に, 、, 祝日, 前, の, 渋谷, の...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dev-s6</td>\n",
       "      <td>今作品では大会でのパラメータUPが非常に大きく、ALL999への育成は回復アイテムを投与しつ...</td>\n",
       "      <td>[今, 作品, で, は, 大会, で, の, パラメータ, UP, が, 非常, に, 大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>dev-s507</td>\n",
       "      <td>お風呂大きくはないけれどお湯はきれい。</td>\n",
       "      <td>[お, 風呂, 大きく, は, ない, けれど, お, 湯, は, きれい, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>dev-s508</td>\n",
       "      <td>新生闇軍団頭領。</td>\n",
       "      <td>[新生, 闇, 軍団, 頭領, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dev-s509</td>\n",
       "      <td>当初の説明と実際の施設の状態が違うという点も,川島町の住民の証言とよく似ています。</td>\n",
       "      <td>[当初, の, 説明, と, 実際, の, 施設, の, 状態, が, 違う, と, いう,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>dev-s510</td>\n",
       "      <td>それでいてこの対応である。</td>\n",
       "      <td>[それ, で, い, て, この, 対応, で, ある, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>dev-s511</td>\n",
       "      <td>児童生徒が犠牲になる事態も多く、同市にある京田幼児園では園舎が倒壊し園児3名が圧死、園児14...</td>\n",
       "      <td>[児童, 生徒, が, 犠牲, に, なる, 事態, も, 多く, 、, 同市, に, ある...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx                                               text  \\\n",
       "0      dev-s1                   ただし、50周年ソングに変更後は、EDも歌つきのものが使われた。   \n",
       "1      dev-s2                       私は初めてだったんだけど思っていたよりも魚は新鮮でした。   \n",
       "2      dev-s4                              セントラル・リーグ審判員の水落朋大は実兄。   \n",
       "3      dev-s5                 多彩なライブアクトとともに、祝日前の渋谷の夜を盛り上げてくれそうだ。   \n",
       "4      dev-s6  今作品では大会でのパラメータUPが非常に大きく、ALL999への育成は回復アイテムを投与しつ...   \n",
       "..        ...                                                ...   \n",
       "496  dev-s507                                お風呂大きくはないけれどお湯はきれい。   \n",
       "497  dev-s508                                           新生闇軍団頭領。   \n",
       "498  dev-s509          当初の説明と実際の施設の状態が違うという点も,川島町の住民の証言とよく似ています。   \n",
       "499  dev-s510                                      それでいてこの対応である。   \n",
       "500  dev-s511  児童生徒が犠牲になる事態も多く、同市にある京田幼児園では園舎が倒壊し園児3名が圧死、園児14...   \n",
       "\n",
       "                                                tokens  \n",
       "0    [ただし, 、, 50, 周年, ソング, に, 変更, 後, は, 、, ED, も, 歌...  \n",
       "1    [私, は, 初めて, だっ, た, ん, だ, けど, 思っ, て, い, た, より, ...  \n",
       "2          [セントラル, ・, リーグ, 審判, 員, の, 水落, 朋大, は, 実兄, 。]  \n",
       "3    [多彩, な, ライブ, アクト, と, とも, に, 、, 祝日, 前, の, 渋谷, の...  \n",
       "4    [今, 作品, で, は, 大会, で, の, パラメータ, UP, が, 非常, に, 大...  \n",
       "..                                                 ...  \n",
       "496          [お, 風呂, 大きく, は, ない, けれど, お, 湯, は, きれい, 。]  \n",
       "497                                 [新生, 闇, 軍団, 頭領, 。]  \n",
       "498  [当初, の, 説明, と, 実際, の, 施設, の, 状態, が, 違う, と, いう,...  \n",
       "499                    [それ, で, い, て, この, 対応, で, ある, 。]  \n",
       "500  [児童, 生徒, が, 犠牲, に, なる, 事態, も, 多く, 、, 同市, に, ある...  \n",
       "\n",
       "[501 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_files = {\"train\": \"train_split.csv\", \"test\": \"test_split.csv\", 'val': 'val_split.csv'}\n",
    "dataset = load_dataset('universal_dependencies', 'ja_gsd')\n",
    "train_split = dataset['train']\n",
    "test_split = dataset['test']\n",
    "val_split = dataset['validation']\n",
    "\n",
    "train_split_df = pd.DataFrame(data=train_split, columns=train_split.features)\n",
    "train_split_df = train_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
    "train_split_df\n",
    "\n",
    "test_split_df = pd.DataFrame(data=test_split, columns=test_split.features)\n",
    "test_split_df = test_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
    "test_split_df\n",
    "\n",
    "val_split_df = pd.DataFrame(data=val_split, columns=val_split.features)\n",
    "val_split_df = val_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
    "val_split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed1afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7027\n",
      "543\n",
      "501\n"
     ]
    }
   ],
   "source": [
    "print(len(train_split))\n",
    "print(len(test_split))\n",
    "print(len(val_split))\n",
    "#train_split['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd62a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kanji data\n",
    "kanji_df = pd.read_json(\"kanji.json\")\n",
    "kanji_df = kanji_df.drop(['strokes', 'grade', 'freq', 'jlpt_old', 'jlpt_new', 'meanings', 'wk_radicals', 'wk_readings_kun', 'wk_readings_on', 'wk_meanings', 'wk_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cdc3535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>kana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-s1</td>\n",
       "      <td>ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に...</td>\n",
       "      <td>[ホッケー, に, は, デンジャラス, プレー, の, 反則, が, ある, の, で, ...</td>\n",
       "      <td>ホッケーにはデンジャラスプレーのはんそくがあるので、しつよりじょうにボールをふかすことはきほ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-s2</td>\n",
       "      <td>また行きたい、そんな気持ちにさせてくれるお店です。</td>\n",
       "      <td>[また, 行き, たい, 、, そんな, 気持ち, に, さ, せ, て, くれる, お, ...</td>\n",
       "      <td>またこうきたい、そんなきじちにさせてくれるおてんです。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-s3</td>\n",
       "      <td>手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻...</td>\n",
       "      <td>[手, に, 持っ, た, 特殊, な, 刃物, を, 使っ, た, アクロバティック, な...</td>\n",
       "      <td>しゅにじったとくしゅなじんぶつをしったアクロバティックなたいじゅつや、よううとはくうどうよう...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-s4</td>\n",
       "      <td>3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。</td>\n",
       "      <td>[3, 年次, に, は, トータル, オフェンス, で, 2, ,, 892, ヤード, ...</td>\n",
       "      <td>3ねんじにはトータルオフェンスで2,892ヤードをかくとくし、これはだいがくきろくとなった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-s5</td>\n",
       "      <td>葬儀の最中ですよ!</td>\n",
       "      <td>[葬儀, の, 最中, です, よ, !]</td>\n",
       "      <td>そうぎのさいちゅうですよ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>train-s7160</td>\n",
       "      <td>「さくらちゃん」と呼ばれている。</td>\n",
       "      <td>[「, さくら, ちゃん, 」, と, 呼ば, れ, て, いる, 。]</td>\n",
       "      <td>「さくらちゃん」とこばれている。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>train-s7161</td>\n",
       "      <td>シェルマルケ氏は新憲法などをめぐり、アハメド暫定大統領と対立していた。</td>\n",
       "      <td>[シェルマルケ, 氏, は, 新, 憲法, など, を, めぐり, 、, アハメド, 暫定,...</td>\n",
       "      <td>シェルマルケしはしんけんほうなどをめぐり、アハメドざんていだいとうりょうとたいりつしていた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7024</th>\n",
       "      <td>train-s7162</td>\n",
       "      <td>自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに,こう語りました。</td>\n",
       "      <td>[自ら, が, オウム, 真理, 教, で, は, ない, 別, の, カルト, 団体, に...</td>\n",
       "      <td>じらがオウムしんりきょうではないべつのカルトだんたいに12ねんかんしょぞくしていたけいけんを...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>train-s7163</td>\n",
       "      <td>紅い髪と同じ色の瞳という容姿に加え、眉と鎖骨の下の部分に二つずつ紅い球体が埋まっているという...</td>\n",
       "      <td>[紅い, 髪, と, 同じ, 色, の, 瞳, と, いう, 容姿, に, 加え, 、, 眉...</td>\n",
       "      <td>こういはつとどうじしょくのどうというようしにかえ、びとさこつのかのぶぶんににつずつこういきゅ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>train-s7164</td>\n",
       "      <td>“人生,棒に振らないように”するためにはカルト団体からの速やかな離脱が必要だと感じるのは記者...</td>\n",
       "      <td>[“, 人生, ,, 棒, に, 振ら, ない, よう, に, ”, する, ため, に, ...</td>\n",
       "      <td>“じんせい,ぼうにしんらないように”するためにはカルトだんたいからのそくやかなりだつがひつよ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7027 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              idx                                               text  \\\n",
       "0        train-s1  ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に...   \n",
       "1        train-s2                          また行きたい、そんな気持ちにさせてくれるお店です。   \n",
       "2        train-s3  手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻...   \n",
       "3        train-s4           3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。   \n",
       "4        train-s5                                          葬儀の最中ですよ!   \n",
       "...           ...                                                ...   \n",
       "7022  train-s7160                                   「さくらちゃん」と呼ばれている。   \n",
       "7023  train-s7161                シェルマルケ氏は新憲法などをめぐり、アハメド暫定大統領と対立していた。   \n",
       "7024  train-s7162     自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに,こう語りました。   \n",
       "7025  train-s7163  紅い髪と同じ色の瞳という容姿に加え、眉と鎖骨の下の部分に二つずつ紅い球体が埋まっているという...   \n",
       "7026  train-s7164  “人生,棒に振らないように”するためにはカルト団体からの速やかな離脱が必要だと感じるのは記者...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [ホッケー, に, は, デンジャラス, プレー, の, 反則, が, ある, の, で, ...   \n",
       "1     [また, 行き, たい, 、, そんな, 気持ち, に, さ, せ, て, くれる, お, ...   \n",
       "2     [手, に, 持っ, た, 特殊, な, 刃物, を, 使っ, た, アクロバティック, な...   \n",
       "3     [3, 年次, に, は, トータル, オフェンス, で, 2, ,, 892, ヤード, ...   \n",
       "4                                 [葬儀, の, 最中, です, よ, !]   \n",
       "...                                                 ...   \n",
       "7022               [「, さくら, ちゃん, 」, と, 呼ば, れ, て, いる, 。]   \n",
       "7023  [シェルマルケ, 氏, は, 新, 憲法, など, を, めぐり, 、, アハメド, 暫定,...   \n",
       "7024  [自ら, が, オウム, 真理, 教, で, は, ない, 別, の, カルト, 団体, に...   \n",
       "7025  [紅い, 髪, と, 同じ, 色, の, 瞳, と, いう, 容姿, に, 加え, 、, 眉...   \n",
       "7026  [“, 人生, ,, 棒, に, 振ら, ない, よう, に, ”, する, ため, に, ...   \n",
       "\n",
       "                                                   kana  \n",
       "0     ホッケーにはデンジャラスプレーのはんそくがあるので、しつよりじょうにボールをふかすことはきほ...  \n",
       "1                           またこうきたい、そんなきじちにさせてくれるおてんです。  \n",
       "2     しゅにじったとくしゅなじんぶつをしったアクロバティックなたいじゅつや、よううとはくうどうよう...  \n",
       "3        3ねんじにはトータルオフェンスで2,892ヤードをかくとくし、これはだいがくきろくとなった。  \n",
       "4                                         そうぎのさいちゅうですよ!  \n",
       "...                                                 ...  \n",
       "7022                                   「さくらちゃん」とこばれている。  \n",
       "7023     シェルマルケしはしんけんほうなどをめぐり、アハメドざんていだいとうりょうとたいりつしていた。  \n",
       "7024  じらがオウムしんりきょうではないべつのカルトだんたいに12ねんかんしょぞくしていたけいけんを...  \n",
       "7025  こういはつとどうじしょくのどうというようしにかえ、びとさこつのかのぶぶんににつずつこういきゅ...  \n",
       "7026  “じんせい,ぼうにしんらないように”するためにはカルトだんたいからのそくやかなりだつがひつよ...  \n",
       "\n",
       "[7027 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As part of data collection, we need a dataset that is ONLY kana. \n",
    "# We could not find a dataset that had both kanji AND kana only version,\n",
    "# so we took a dataset and converted the kanji to kana. \n",
    "# kanji to kana is less ambiguous with how on vs kun readings work, but there is still some abiguity.\n",
    "# we recognize that here, in the dataset.\n",
    "def unkanjify(df):\n",
    "    new_sent = []\n",
    "    for word in df['tokens']:\n",
    "        #iterating list of chararacters\n",
    "        for char in word:\n",
    "            if char in kanji_df.columns:#is kanji\n",
    "                if char in kanji_df.columns or char in kanji_df.columns:\n",
    "                    if kanji_df[char].loc['readings_on']:\n",
    "                        new_sent.append(kanji_df[char].loc['readings_on'][0])\n",
    "                    else:\n",
    "                        new_sent.append(char)\n",
    "                #kun reading\n",
    "                else:\n",
    "                    for c in kanji_df[char].loc['readings_kun'][0]:\n",
    "                        if c == '.':\n",
    "                            continue\n",
    "                        new_sent.append(c)\n",
    "            else:#is not kanji\n",
    "                new_sent.append(char)\n",
    "    joined_sent = ''.join(new_sent)\n",
    "    return joined_sent\n",
    "\n",
    "# Applying the function to all splits (train, test and val)\n",
    "train_split_df['kana'] = train_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
    "test_split_df['kana'] = test_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
    "val_split_df['kana'] = val_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
    "train_split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9b69a",
   "metadata": {},
   "source": [
    "# download and intialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58361eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d4810c90a54cc896a0d684bd7e370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/545M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlaw5\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wlaw5\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "model = TFBertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be746535",
   "metadata": {},
   "source": [
    "# Concatinate the two stirngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "kana = \"one two thirty-four\"\n",
    "kanji = \"1 2 34\"\n",
    "\n",
    "\"one two thirty-four\" + \" [SEP] \" + \"1 2 34\"\n",
    "kana + \" [SEP] \" + kanji\n",
    "\n",
    "\"one two thirty-four [SEP] 1 2 34\"\n",
    "3 = [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61215ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に...\n",
       "1       また行きたい、そんな気持ちにさせてくれるお店です。[SEP]またこうきたい、そんなきじちにさ...\n",
       "2       手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻...\n",
       "3       3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。[SEP]3...\n",
       "4                             葬儀の最中ですよ![SEP]そうぎのさいちゅうですよ!\n",
       "                              ...                        \n",
       "7022                「さくらちゃん」と呼ばれている。[SEP]「さくらちゃん」とこばれている。\n",
       "7023    シェルマルケ氏は新憲法などをめぐり、アハメド暫定大統領と対立していた。[SEP]シェルマルケ...\n",
       "7024    自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに,こう語りました。...\n",
       "7025    紅い髪と同じ色の瞳という容姿に加え、眉と鎖骨の下の部分に二つずつ紅い球体が埋まっているという...\n",
       "7026    “人生,棒に振らないように”するためにはカルト団体からの速やかな離脱が必要だと感じるのは記者...\n",
       "Name: comibned, Length: 7027, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_df['comibned']= train_split_df.text + \"[SEP]\" + train_split_df.kana\n",
    "train_split_df.comibned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a551ff9",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "convert everything into appropriate format for tenosrflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2872024b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[    2, 24543, 24893, ...,     0,     0,     0],\n",
       "       [    2,   106,    77, ...,     0,     0,     0],\n",
       "       [    2,   319,     7, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    2,    98,    60, ...,     0,     0,     0],\n",
       "       [    2,  4028,    21, ...,     0,     0,     0],\n",
       "       [    2,  2203,    53, ...,     0,     0,     0]]), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(train_split_df.comibned.values.tolist(),max_length=512,truncation=True,padding='max_length',return_tensors='np')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5982e",
   "metadata": {},
   "source": [
    "# save the kanji text before we mask, on a seprate varibal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06d3ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs['input_ids']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075615f1",
   "metadata": {},
   "source": [
    "# MASK kanji text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "the apple [sep] fell down on the chari\n",
    "1  2   64   3    7    234  34  75  12\n",
    "\n",
    "\n",
    "the [sep][MASK] \n",
    "1   3   37   0   0 0   0   0  0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da09788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inp_ids = []\n",
    "inp_ids = []\n",
    "idx = 0\n",
    "for inp in inputs.input_ids:\n",
    "    \n",
    "    inp = np.array(inp)\n",
    "    \n",
    "    locatio_of_sep = np.where(inp == 3)[0][0]\n",
    "    \n",
    "    \n",
    "    actual_tokens = list(set(range(locatio_of_sep,inp.shape[0])) - \n",
    "                         set(np.where((inp == 3) \n",
    "                            | (inp == 0))[0].tolist()))\n",
    "    #We need to select 15% random tokens from the given list\n",
    "    num_of_token_to_mask = int(len(actual_tokens)*0.50)\n",
    "    token_to_mask = np.random.choice(np.array(actual_tokens), \n",
    "                                     size=num_of_token_to_mask, \n",
    "                                     replace=False).tolist()\n",
    "    #Now we have the indices where we need to mask the tokens\n",
    "    inp[token_to_mask] = 103\n",
    "    inp_ids.append(inp)\n",
    "    idx += 1\n",
    "#inp_ids = tf.convert_to_tensor(inp_ids)\n",
    "inputs['input_ids'] = np.array(inp_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9ece202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c12e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "879/879 [==============================] - 14859s 17s/step - loss: 0.0514\n",
      "Epoch 2/10\n",
      "879/879 [==============================] - 15267s 17s/step - loss: 0.0191\n",
      "Epoch 3/10\n",
      "879/879 [==============================] - 14968s 17s/step - loss: 0.0126\n",
      "Epoch 4/10\n",
      "879/879 [==============================] - 14866s 17s/step - loss: 0.0112\n",
      "Epoch 5/10\n",
      "879/879 [==============================] - 14990s 17s/step - loss: 0.0086\n",
      "Epoch 6/10\n",
      "879/879 [==============================] - 14660s 17s/step - loss: 0.0085\n",
      "Epoch 7/10\n",
      "879/879 [==============================] - 14769s 17s/step - loss: 0.0073\n",
      "Epoch 8/10\n",
      "879/879 [==============================] - 14760s 17s/step - loss: 0.0059\n",
      "Epoch 9/10\n",
      "879/879 [==============================] - 14730s 17s/step - loss: 0.0055\n",
      "Epoch 10/10\n",
      "879/879 [==============================] - 14691s 17s/step - loss: 0.0062\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "history = model.fit([inputs.input_ids,inputs.attention_mask],inputs.labels,verbose=1,batch_size=8,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd15af3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kana_to_kanji\\\\tokenizer_config.json',\n",
       " 'kana_to_kanji\\\\special_tokens_map.json',\n",
       " 'kana_to_kanji\\\\vocab.txt',\n",
       " 'kana_to_kanji\\\\added_tokens.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"kana_to_kanji\")\n",
    "tokenizer.save_pretrained(\"kana_to_kanji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c08e2",
   "metadata": {},
   "source": [
    "# run it, convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "514429bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at kana_to_kanji.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"kana_to_kanji\")\n",
    "model = TFBertForMaskedLM.from_pretrained(\"kana_to_kanji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "testColumn + '[SEP]' +   \"[MASK]\"*150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12a8fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 自 らがオウム 真 理 教 ではない 別 のカルト 団 体 に12 年 間 所 属 していた 経 [UNK] をもとに [SEP] じらしょじいる 。 [SEP]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentance = \"自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
    "inp = tokenizer(sentance,return_tensors='np')\n",
    "mask_loc = np.where(inp.input_ids[0] == 103)[0].tolist()\n",
    "out = model(inp).logits[0]\n",
    "predicted_tokens = np.argmax(out[mask_loc],axis=1).tolist()\n",
    "tokenizer.decode(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fb69de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないよう...\n",
       "1      幸福の科学側からは,特にどうしてほしいという要望はいただいていません。[SEP][MASK]...\n",
       "2      星取り参加は当然とされ,不参加は白眼視される。[SEP][MASK][MASK][MASK]...\n",
       "3      室長の対応には終始誠実さが感じられた。[SEP][MASK][MASK][MASK][MAS...\n",
       "4      多くの女性が生理のことで悩んでいます。[SEP][MASK][MASK][MASK][MAS...\n",
       "                             ...                        \n",
       "538    プラスミドを改変してベクターとして利用する。[SEP][MASK][MASK][MASK][...\n",
       "539    駐車場は狭いので混みあう時間帯は注意ですが、事前に電話をして到着時間を伝えておけば良いですね...\n",
       "540    散髪のやり方次第で頭脳は発達すると考えて、どの店の散髪がよいか理髪店を絶えず替えていた。[S...\n",
       "541    見上げる程の大きさの弧形が地表に露出しているが、これですら全体の一部でしかなく、全体は地殻を...\n",
       "542    また、顕彰当時現役の選手も存在したが、いずれも既に全女を退団していた。[SEP][MASK]...\n",
       "Name: masked_text, Length: 543, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split_df['masked_text'] = test_split_df.text + '[SEP]' +   \"[MASK]\"*150\n",
    "test_split_df['masked_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f60492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlaw5\\AppData\\Local\\Temp\\ipykernel_332824\\492050940.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for sentance in tqdm(test_split_df['masked_text'].values.tolist()[:20]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ed326896484b6096ef1cc8f96e679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。',\n",
       " '幸福の科学側からは,特にどうしてほしいという要望はいただいていません。',\n",
       " '星取り参加は当然とされ,不参加は白眼視される。',\n",
       " '室長の対応には終始誠実さが感じられた。',\n",
       " '多くの女性が生理のことで悩んでいます。',\n",
       " '先生の理想は限りなく高い。',\n",
       " 'それは兎も角,私も明日の社説を楽しみにしております。',\n",
       " 'そうだったらいいなあとは思いますが,日本学術会議の会長談話について“当会では,標記の件について,以下のように考えます。”',\n",
       " '教団にとっては存続が厳しくなると思う。',\n",
       " 'しかし強制していなくても問題です',\n",
       " '民族派のみなさんにとって[UNK]下はいちばん大切な方ですから,その霊言について“あり得ない”という前提でお話をされる。',\n",
       " '新しい産業構造を作らなければいけない。',\n",
       " '心がないんだ。',\n",
       " 'そのモサは今何をしているか。',\n",
       " '本音としてはこの火に油を注ぎたいけれど。',\n",
       " '25日も楽しみにされてください。',\n",
       " '[UNK]~,わが署に来たのなら教[UNK]~。',\n",
       " 'どの本もツッコミどころが山ほどあり,会場内が爆笑の連続でした。',\n",
       " '社会への恨みと,破滅させたいという恐ろしいほどの煩悩。',\n",
       " 'とんでもない。']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_sentance_list = []\n",
    "for sentance in tqdm(test_split_df['masked_text'].values.tolist()[:20]):\n",
    "    #sentance = \"自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
    "    inp = tokenizer(sentance,return_tensors='np')\n",
    "    mask_loc = np.where(inp.input_ids[0] == 103)[0].tolist()\n",
    "    out = model(inp).logits[0]\n",
    "    predicted_tokens = np.argmax(out[mask_loc],axis=1).tolist()\n",
    "    converted_sentance = tokenizer.decode(predicted_tokens)\n",
    "    converted_sentance_list.append(converted_sentance.split('[SEP]')[0].strip('[CLS]').replace(\" \",\"\"))\n",
    "\n",
    "converted_sentance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "なんだよなんだよぉ~,わが署に来たのなら教えてくれよぉ~。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d6e2eed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['これにふかいかんをじすじゅうみんはいましたが,げんざい,ひょうりつってはんたいやこうぎのせいをきょげているじゅうみんはいないようです。',\n",
       " 'こうふくのかがくそくからは,とくにどうしてほしいというようぼうはいただいていません。',\n",
       " 'せいしゅりさんかはとうぜんとされ,ふさんかははくがんしされる。',\n",
       " 'しつちょうのたいおうにはしゅうしせいじつさがかんじられた。',\n",
       " 'たくのじょせいがせいりのことでのうんでいます。',\n",
       " 'せんせいのりそうはげんりなくこうい。',\n",
       " 'それはともかく,しもめいにちのしゃせつをがくしみにしております。',\n",
       " 'そうだったらいいなあとはしいますが,にちほんがくじゅつかいぎのかいちょうだんわについて“とうかいでは,ひょうきのけんについて,いかのようにこうえます。”',\n",
       " 'きょうだんにとってはそんぞくがげんしくなるとしう。',\n",
       " 'しかしきょうせいしていなくてももんだいです',\n",
       " 'みんぞくはのみなさんにとってへいかはいちばんだいせつなほうですから,そのれいげんについて“ありとくない”というぜんていでおわをされる。',\n",
       " 'しんしいさんぎょうこうぞうをさくらなければいけない。',\n",
       " 'しんがないんだ。',\n",
       " 'そのモサはこんかをしているか。',\n",
       " 'ほんおんとしてはこのかにゆをちゅうぎたいけれど。',\n",
       " '25にちもがくしみにされてください。',\n",
       " 'なんだよなんだよぉ~,わがしょにらいたのならきょうえてくれよぉ~。',\n",
       " 'どのほんもツッコミどころがさんほどあり,かいじょうないがばくしょうのれんぞくでした。',\n",
       " 'しゃかいへのこんみと,はめつさせたいというきょうろしいほどのはんのう。',\n",
       " 'とんでもない。']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split_df.kana[:20].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66c30c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。',\n",
       " '幸福の科学側からは,特にどうしてほしいという要望はいただいていません。',\n",
       " '星取り参加は当然とされ,不参加は白眼視される。',\n",
       " '室長の対応には終始誠実さが感じられた。',\n",
       " '多くの女性が生理のことで悩んでいます。',\n",
       " '先生の理想は限りなく高い。',\n",
       " 'それは兎も角,私も明日の社説を楽しみにしております。',\n",
       " 'そうだったらいいなあとは思いますが,日本学術会議の会長談話について“当会では,標記の件について,以下のように考えます。”',\n",
       " '教団にとっては存続が厳しくなると思う。',\n",
       " 'しかし強制していなくても問題です',\n",
       " '民族派のみなさんにとって陛下はいちばん大切な方ですから,その霊言について“あり得ない”という前提でお話をされる。',\n",
       " '新しい産業構造を作らなければいけない。',\n",
       " '心がないんだ。',\n",
       " 'そのモサは今何をしているか。',\n",
       " '本音としてはこの火に油を注ぎたいけれど。',\n",
       " '25日も楽しみにされてください。',\n",
       " 'なんだよなんだよぉ~,わが署に来たのなら教えてくれよぉ~。',\n",
       " 'どの本もツッコミどころが山ほどあり,会場内が爆笑の連続でした。',\n",
       " '社会への恨みと,破滅させたいという恐ろしいほどの煩悩。',\n",
       " 'とんでもない。']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split_df.text[:20].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12475fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
