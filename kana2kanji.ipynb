{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdda2f3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from fugashi import Tagger\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import unicodedata\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31988610",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082cad4",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Converting test, train and val datasets which include kanji to be purely kana (furigana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a864398",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#data_files = {\"train\": \"train_split.csv\", \"test\": \"test_split.csv\", 'val': 'val_split.csv'}\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniversal_dependencies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mja_gsd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m train_split \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m test_split \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#data_files = {\"train\": \"train_split.csv\", \"test\": \"test_split.csv\", 'val': 'val_split.csv'}\n",
    "dataset = load_dataset('universal_dependencies', 'ja_gsd')\n",
    "train_split = dataset['train']\n",
    "test_split = dataset['test']\n",
    "val_split = dataset['validation']\n",
    "\n",
    "train_split_df = pd.DataFrame(data=train_split, columns=train_split.features)\n",
    "train_split_df = train_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
    "train_split_df\n",
    "\n",
    "test_split_df = pd.DataFrame(data=test_split, columns=test_split.features)\n",
    "test_split_df = test_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
    "test_split_df\n",
    "\n",
    "val_split_df = pd.DataFrame(data=val_split, columns=val_split.features)\n",
    "val_split_df = val_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
    "val_split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed1afdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_split\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_split))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_split))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_split' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(train_split))\n",
    "print(len(test_split))\n",
    "print(len(val_split))\n",
    "#train_split['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd62a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kanji data\n",
    "kanji_df = pd.read_json(\"kanji.json\")\n",
    "kanji_df = kanji_df.drop(['strokes', 'grade', 'freq', 'jlpt_old', 'jlpt_new', 'meanings', 'wk_radicals', 'wk_readings_kun', 'wk_readings_on', 'wk_meanings', 'wk_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cdc3535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>kana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-s1</td>\n",
       "      <td>ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に...</td>\n",
       "      <td>[ホッケー, に, は, デンジャラス, プレー, の, 反則, が, ある, の, で, ...</td>\n",
       "      <td>ホッケーにはデンジャラスプレーのはんそくがあるので、しつよりじょうにボールをふかすことはきほ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-s2</td>\n",
       "      <td>また行きたい、そんな気持ちにさせてくれるお店です。</td>\n",
       "      <td>[また, 行き, たい, 、, そんな, 気持ち, に, さ, せ, て, くれる, お, ...</td>\n",
       "      <td>またこうきたい、そんなきじちにさせてくれるおてんです。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-s3</td>\n",
       "      <td>手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻...</td>\n",
       "      <td>[手, に, 持っ, た, 特殊, な, 刃物, を, 使っ, た, アクロバティック, な...</td>\n",
       "      <td>しゅにじったとくしゅなじんぶつをしったアクロバティックなたいじゅつや、よううとはくうどうよう...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-s4</td>\n",
       "      <td>3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。</td>\n",
       "      <td>[3, 年次, に, は, トータル, オフェンス, で, 2, ,, 892, ヤード, ...</td>\n",
       "      <td>3ねんじにはトータルオフェンスで2,892ヤードをかくとくし、これはだいがくきろくとなった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-s5</td>\n",
       "      <td>葬儀の最中ですよ!</td>\n",
       "      <td>[葬儀, の, 最中, です, よ, !]</td>\n",
       "      <td>そうぎのさいちゅうですよ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>train-s7160</td>\n",
       "      <td>「さくらちゃん」と呼ばれている。</td>\n",
       "      <td>[「, さくら, ちゃん, 」, と, 呼ば, れ, て, いる, 。]</td>\n",
       "      <td>「さくらちゃん」とこばれている。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>train-s7161</td>\n",
       "      <td>シェルマルケ氏は新憲法などをめぐり、アハメド暫定大統領と対立していた。</td>\n",
       "      <td>[シェルマルケ, 氏, は, 新, 憲法, など, を, めぐり, 、, アハメド, 暫定,...</td>\n",
       "      <td>シェルマルケしはしんけんほうなどをめぐり、アハメドざんていだいとうりょうとたいりつしていた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7024</th>\n",
       "      <td>train-s7162</td>\n",
       "      <td>自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに,こう語りました。</td>\n",
       "      <td>[自ら, が, オウム, 真理, 教, で, は, ない, 別, の, カルト, 団体, に...</td>\n",
       "      <td>じらがオウムしんりきょうではないべつのカルトだんたいに12ねんかんしょぞくしていたけいけんを...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>train-s7163</td>\n",
       "      <td>紅い髪と同じ色の瞳という容姿に加え、眉と鎖骨の下の部分に二つずつ紅い球体が埋まっているという...</td>\n",
       "      <td>[紅い, 髪, と, 同じ, 色, の, 瞳, と, いう, 容姿, に, 加え, 、, 眉...</td>\n",
       "      <td>こういはつとどうじしょくのどうというようしにかえ、びとさこつのかのぶぶんににつずつこういきゅ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>train-s7164</td>\n",
       "      <td>“人生,棒に振らないように”するためにはカルト団体からの速やかな離脱が必要だと感じるのは記者...</td>\n",
       "      <td>[“, 人生, ,, 棒, に, 振ら, ない, よう, に, ”, する, ため, に, ...</td>\n",
       "      <td>“じんせい,ぼうにしんらないように”するためにはカルトだんたいからのそくやかなりだつがひつよ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7027 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              idx                                               text  \\\n",
       "0        train-s1  ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に...   \n",
       "1        train-s2                          また行きたい、そんな気持ちにさせてくれるお店です。   \n",
       "2        train-s3  手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻...   \n",
       "3        train-s4           3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。   \n",
       "4        train-s5                                          葬儀の最中ですよ!   \n",
       "...           ...                                                ...   \n",
       "7022  train-s7160                                   「さくらちゃん」と呼ばれている。   \n",
       "7023  train-s7161                シェルマルケ氏は新憲法などをめぐり、アハメド暫定大統領と対立していた。   \n",
       "7024  train-s7162     自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに,こう語りました。   \n",
       "7025  train-s7163  紅い髪と同じ色の瞳という容姿に加え、眉と鎖骨の下の部分に二つずつ紅い球体が埋まっているという...   \n",
       "7026  train-s7164  “人生,棒に振らないように”するためにはカルト団体からの速やかな離脱が必要だと感じるのは記者...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [ホッケー, に, は, デンジャラス, プレー, の, 反則, が, ある, の, で, ...   \n",
       "1     [また, 行き, たい, 、, そんな, 気持ち, に, さ, せ, て, くれる, お, ...   \n",
       "2     [手, に, 持っ, た, 特殊, な, 刃物, を, 使っ, た, アクロバティック, な...   \n",
       "3     [3, 年次, に, は, トータル, オフェンス, で, 2, ,, 892, ヤード, ...   \n",
       "4                                 [葬儀, の, 最中, です, よ, !]   \n",
       "...                                                 ...   \n",
       "7022               [「, さくら, ちゃん, 」, と, 呼ば, れ, て, いる, 。]   \n",
       "7023  [シェルマルケ, 氏, は, 新, 憲法, など, を, めぐり, 、, アハメド, 暫定,...   \n",
       "7024  [自ら, が, オウム, 真理, 教, で, は, ない, 別, の, カルト, 団体, に...   \n",
       "7025  [紅い, 髪, と, 同じ, 色, の, 瞳, と, いう, 容姿, に, 加え, 、, 眉...   \n",
       "7026  [“, 人生, ,, 棒, に, 振ら, ない, よう, に, ”, する, ため, に, ...   \n",
       "\n",
       "                                                   kana  \n",
       "0     ホッケーにはデンジャラスプレーのはんそくがあるので、しつよりじょうにボールをふかすことはきほ...  \n",
       "1                           またこうきたい、そんなきじちにさせてくれるおてんです。  \n",
       "2     しゅにじったとくしゅなじんぶつをしったアクロバティックなたいじゅつや、よううとはくうどうよう...  \n",
       "3        3ねんじにはトータルオフェンスで2,892ヤードをかくとくし、これはだいがくきろくとなった。  \n",
       "4                                         そうぎのさいちゅうですよ!  \n",
       "...                                                 ...  \n",
       "7022                                   「さくらちゃん」とこばれている。  \n",
       "7023     シェルマルケしはしんけんほうなどをめぐり、アハメドざんていだいとうりょうとたいりつしていた。  \n",
       "7024  じらがオウムしんりきょうではないべつのカルトだんたいに12ねんかんしょぞくしていたけいけんを...  \n",
       "7025  こういはつとどうじしょくのどうというようしにかえ、びとさこつのかのぶぶんににつずつこういきゅ...  \n",
       "7026  “じんせい,ぼうにしんらないように”するためにはカルトだんたいからのそくやかなりだつがひつよ...  \n",
       "\n",
       "[7027 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As part of data collection, we need a dataset that is ONLY kana. \n",
    "# We could not find a dataset that had both kanji AND kana only version,\n",
    "# so we took a dataset and converted the kanji to kana. \n",
    "# kanji to kana is less ambiguous with how on vs kun readings work, but there is still some abiguity.\n",
    "# we recognize that here, in the dataset.\n",
    "def unkanjify(df):\n",
    "    new_sent = []\n",
    "    for word in df['tokens']:\n",
    "        #iterating list of chararacters\n",
    "        for char in word:\n",
    "            if char in kanji_df.columns:#is kanji\n",
    "                if char in kanji_df.columns or char in kanji_df.columns:\n",
    "                    if kanji_df[char].loc['readings_on']:\n",
    "                        new_sent.append(kanji_df[char].loc['readings_on'][0])\n",
    "                    else:\n",
    "                        new_sent.append(char)\n",
    "                #kun reading\n",
    "                else:\n",
    "                    for c in kanji_df[char].loc['readings_kun'][0]:\n",
    "                        if c == '.':\n",
    "                            continue\n",
    "                        new_sent.append(c)\n",
    "            else:#is not kanji\n",
    "                new_sent.append(char)\n",
    "    joined_sent = ''.join(new_sent)\n",
    "    return joined_sent\n",
    "\n",
    "# Applying the function to all splits (train, test and val)\n",
    "train_split_df['kana'] = train_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
    "test_split_df['kana'] = test_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
    "val_split_df['kana'] = val_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
    "train_split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6017b0",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912753c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kana is Y\n",
    "#text is X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3702ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0a18118",
   "metadata": {},
   "source": [
    "tokoenie kona\n",
    "kona_tokeniz = bert.tokenizer('ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に')\n",
    "input_ids: [[101, 2040, 2001, 4116, 9762, 1029, 102, 4938, 3487, 9762, 2124, 2004, 4116, 9762, 2003, 1037, 7214, 2839, 1999, 14936, 102]]\n",
    "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "\n",
    "\n",
    "tokenize konji\n",
    "bert.tokeinizer('ホッケーにはデンジャラスプレーのはんそくがあるので、しつよりじょうにボールをふかすことはきほ.')\n",
    "input_ids: [[101, 2040, 4821, 3849, 4831, 5331, 2382, 4938, 34313, 9762, 2124, 2004, 3491, 37183, 2003, 1037, 38193, 2839, 1838, 3781, 102]]\n",
    "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "randomly replace kona(orignal sentance) tokenize with konji('output setnacne') tokens (do ~15%)\n",
    "                          V                 v\t\t\t\t\t                     v\t\t\t\t\t\tv\t\n",
    "input_ids: [[101, 2040, 4821, 4116, 9762, 1029, 102, 4938, 3487, 9762, 2124, 2004, 4116, 9762, 2003, 1037, 7214, 2839, 2839, 14936, 102]]\n",
    "attention_mask: [[1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "\n",
    "\n",
    "done!\n",
    "train your model and predict on testing data\n",
    "\n",
    "\n",
    "take the length of list, then take numpy random indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58361eee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl-tohoku/bert-base-japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb9201e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputsKanji \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(train_split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m],max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m inputsKana \u001b[38;5;241m=\u001b[39m tokenizer(train_split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkana\u001b[39m\u001b[38;5;124m'\u001b[39m],max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "inputsKanji = tokenizer(train_split_df['tokens'],max_length=100,truncation=True,padding='max_length',return_tensors='tf')\n",
    "inputsKana = tokenizer(train_split_df['kana'],max_length=100,truncation=True,padding='max_length',return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0009cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsKana['labels'] = inputsKanji['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ids = []\n",
    "lbs = []\n",
    "idx = 0\n",
    "for inp in inputsKana.input_ids.numpy():\n",
    "    actual_tokens = list(set(range(100)) - \n",
    "                         set(np.where((inp == 101) | (inp == 102) \n",
    "                            | (inp == 0))[0].tolist()))\n",
    "    #We need to select 15% random tokens from the given list\n",
    "    num_of_token_to_mask = int(len(actual_tokens)*0.15)\n",
    "    token_to_mask = np.random.choice(np.array(actual_tokens), \n",
    "                                     size=num_of_token_to_mask, \n",
    "                                     replace=False).tolist()\n",
    "    #Now we have the indices where we need to mask the tokens\n",
    "    inp[token_to_mask] = 103\n",
    "    inp_ids.append(inp)\n",
    "    idx += 1\n",
    "inp_ids = tf.convert_to_tensor(inp_ids)\n",
    "inputsKana['input_ids'] = inp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330a1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text\n",
    "        self.targets = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, NUM_OUT):\n",
    "        super(BERTClass, self).__init__()\n",
    "                   \n",
    "        self.l1 = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")#BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 32)\n",
    "        self.classifier = torch.nn.Linear(32, NUM_OUT)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "#         pooler = torch.nn.Tanh()(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "        \n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "def train(model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    for data in tqdm(training_loader):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss\n",
    "    \n",
    "def validation(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testing_loader):\n",
    "            targets = data['targets']\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach()\n",
    "            fin_outputs.extend(outputs)\n",
    "            fin_targets.extend(targets)\n",
    "    return torch.stack(fin_outputs), torch.stack(fin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a802aa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m NUM_OUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;66;03m# binary task\u001b[39;00m\n\u001b[0;32m      6\u001b[0m LEARNING_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-05\u001b[39m\n\u001b[1;32m----> 8\u001b[0m training_data \u001b[38;5;241m=\u001b[39m MultiLabelDataset(\u001b[43mtrain_X\u001b[49m, train_y, tokenizer, MAX_LEN)\n\u001b[0;32m      9\u001b[0m test_data \u001b[38;5;241m=\u001b[39m MultiLabelDataset(test_X, test_y, tokenizer, MAX_LEN)\n\u001b[0;32m     11\u001b[0m train_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: BATCH_SIZE,\n\u001b[0;32m     12\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m                 }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "#Vars\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "NUM_OUT = 7 # binary task\n",
    "LEARNING_RATE = 2e-05\n",
    "\n",
    "training_data = MultiLabelDataset(train_X, train_y, tokenizer, MAX_LEN)\n",
    "test_data = MultiLabelDataset(test_X, test_y, tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }    \n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, **train_params)\n",
    "testing_loader = torch.utils.data.DataLoader(test_data, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClass(NUM_OUT)\n",
    "model.to(device)    \n",
    "\n",
    "optimizer_bert = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12e646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2d735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8fe08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
