{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets conllu"
      ],
      "metadata": {
        "id": "r9bXkDQx6fZ7"
      },
      "id": "r9bXkDQx6fZ7",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fdda2f3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fdda2f3b",
        "outputId": "529920df-6539-403c-9c8f-4a6039adbfba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import unicodedata\n",
        "from datasets import load_dataset\n",
        "import itertools\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import transformers\n",
        "\n",
        "import torch\n",
        "from torch import cuda\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b082cad4",
      "metadata": {
        "id": "b082cad4"
      },
      "source": [
        "# The Data\n",
        "\n",
        "Converting test, train and val datasets which include kanji to be purely kana (furigana)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a864398",
      "metadata": {
        "id": "6a864398"
      },
      "outputs": [],
      "source": [
        "data_files = {\"train\": \"train_split.csv\", \"test\": \"test_split.csv\", 'val': 'val_split.csv'}\n",
        "# df = pd.DataFrame([['ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に','ホッケーにはデンジャラスプレーのはんそくがあるので、しつよりじょうにボールをふかすことはきほ'],\n",
        "#              ['また行きたい、そんな気持ちにさせてくれるお店です。','またこうきたい、そんなきじちにさせてくれるおてんです'],\n",
        "#              ['手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻','しゅにじったとくしゅなじんぶつをしったアクロバティックなたいじゅつや、よううとはくうどうよう'],\n",
        "#              ['3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。','3ねんじにはトータルオフェンスで2,892ヤードをかくとくし、これはだいがくきろくとなった。']],\n",
        "#              columns = ['text','kana'])\n",
        "dataset = load_dataset('universal_dependencies', 'ja_gsd')\n",
        "train_split = dataset['train']\n",
        "test_split = dataset['test']\n",
        "val_split = dataset['validation']\n",
        "train_split = df\n",
        "test_split = df\n",
        "val_split = df\n",
        "\n",
        "#train_split_df = pd.DataFrame(data=train_split, columns=train_split.features)\n",
        "train_split_df = train_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
        "train_split_df\n",
        "\n",
        "test_split_df = pd.DataFrame(data=test_split, columns=test_split.features)\n",
        "test_split_df = test_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
        "test_split_df\n",
        "\n",
        "val_split_df = pd.DataFrame(data=val_split, columns=val_split.features)\n",
        "val_split_df = val_split_df.drop(['lemmas','upos','xpos','feats','head','deprel','deps','misc'], axis=1)\n",
        "val_split_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed1afdc",
      "metadata": {
        "id": "3ed1afdc"
      },
      "outputs": [],
      "source": [
        "print(len(train_split))\n",
        "print(len(test_split))\n",
        "print(len(val_split))\n",
        "#train_split['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd62a80",
      "metadata": {
        "id": "afd62a80"
      },
      "outputs": [],
      "source": [
        "#load kanji data\n",
        "kanji_df = pd.read_json(\"kanji.json\")\n",
        "kanji_df = kanji_df.drop(['strokes', 'grade', 'freq', 'jlpt_old', 'jlpt_new', 'meanings', 'wk_radicals', 'wk_readings_kun', 'wk_readings_on', 'wk_meanings', 'wk_level'], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdc3535",
      "metadata": {
        "scrolled": true,
        "id": "1cdc3535"
      },
      "outputs": [],
      "source": [
        "# As part of data collection, we need a dataset that is ONLY kana. \n",
        "# We could not find a dataset that had both kanji AND kana only version,\n",
        "# so we took a dataset and converted the kanji to kana. \n",
        "# kanji to kana is less ambiguous with how on vs kun readings work, but there is still some abiguity.\n",
        "# we recognize that here, in the dataset.\n",
        "def unkanjify(df):\n",
        "    new_sent = []\n",
        "    for word in df['tokens']:\n",
        "        #iterating list of chararacters\n",
        "        for char in word:\n",
        "            if char in kanji_df.columns:#is kanji\n",
        "                if char in kanji_df.columns or char in kanji_df.columns:\n",
        "                    if kanji_df[char].loc['readings_on']:\n",
        "                        new_sent.append(kanji_df[char].loc['readings_on'][0])\n",
        "                    else:\n",
        "                        new_sent.append(char)\n",
        "                #kun reading\n",
        "                else:\n",
        "                    for c in kanji_df[char].loc['readings_kun'][0]:\n",
        "                        if c == '.':\n",
        "                            continue\n",
        "                        new_sent.append(c)\n",
        "            else:#is not kanji\n",
        "                new_sent.append(char)\n",
        "    joined_sent = ''.join(new_sent)\n",
        "    return joined_sent\n",
        "\n",
        "# Applying the function to all splits (train, test and val)\n",
        "train_split_df['kana'] = train_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
        "test_split_df['kana'] = test_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
        "val_split_df['kana'] = val_split_df.apply(lambda x: unkanjify(x), axis=1) \n",
        "train_split_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac7464e1",
      "metadata": {
        "id": "ac7464e1"
      },
      "source": [
        "# download and intialize models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58361eee",
      "metadata": {
        "scrolled": true,
        "id": "58361eee"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForMaskedLM\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "model = TFBertForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be746535",
      "metadata": {
        "id": "be746535"
      },
      "source": [
        "# Concatinate the two stirngs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` kana = \"one two thirty-four\"\n",
        "kanji = \"1 2 34\"\n",
        "\n",
        "\"one two thirty-four\" + \" [SEP] \" + \"1 2 34\"\n",
        "kana + \" [SEP] \" + kanji\n",
        "\n",
        "\"one two thirty-four [SEP] 1 2 34\"\n",
        "3 = [SEP]\n",
        "```"
      ],
      "metadata": {
        "id": "B-lnG-Umey2D"
      },
      "id": "B-lnG-Umey2D"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "61215ef5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61215ef5",
        "outputId": "27f42a71-12bb-4fa8-cba3-bbdb44332e1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則に...\n",
              "1    また行きたい、そんな気持ちにさせてくれるお店です。[SEP]またこうきたい、そんなきじちにさ...\n",
              "2    手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻...\n",
              "3    3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。[SEP]3...\n",
              "Name: comibned, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "train_split_df['comibned']= train_split_df.text + \"[SEP]\" + train_split_df.kana\n",
        "train_split_df.comibned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "679178f6",
      "metadata": {
        "id": "679178f6"
      },
      "source": [
        "# Tokenize\n",
        "\n",
        "convert everything into appropriate format for tenosrflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "2872024b",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2872024b",
        "outputId": "a086bf7c-7aaf-4888-b17c-35ecb8fa81bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': array([[    2, 24543, 24893, ...,     0,     0,     0],\n",
              "       [    2,   106,    77, ...,     0,     0,     0],\n",
              "       [    2,   319,     7, ...,     0,     0,     0],\n",
              "       [    2,    48,    19, ...,     0,     0,     0]]), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "inputs = tokenizer(train_split_df.comibned.values.tolist(),max_length=512,truncation=True,padding='max_length',return_tensors='np')\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e4bd5a",
      "metadata": {
        "id": "49e4bd5a"
      },
      "source": [
        "# save the kanji text before we mask, on a seprate varibal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "42061162",
      "metadata": {
        "id": "42061162"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs['input_ids']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18f8029",
      "metadata": {
        "id": "b18f8029"
      },
      "source": [
        "# MASK kanji text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b05003",
      "metadata": {
        "id": "46b05003"
      },
      "source": [
        "the apple [sep] fell down on the chari\n",
        "1  2   64   3    7    234  34  75  12\n",
        "\n",
        "\n",
        "the [sep][MASK] \n",
        "1   3   37   0   0 0   0   0  0 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pv3annjae8ek"
      },
      "id": "pv3annjae8ek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "da09788a",
      "metadata": {
        "id": "da09788a"
      },
      "outputs": [],
      "source": [
        "### inp_ids = []\n",
        "inp_ids = []\n",
        "idx = 0\n",
        "for inp in inputs.input_ids:\n",
        "    \n",
        "    inp = np.array(inp)\n",
        "    \n",
        "    locatio_of_sep = np.where(inp == 3)[0][0]\n",
        "    \n",
        "    \n",
        "    actual_tokens = list(set(range(locatio_of_sep,inp.shape[0])) - \n",
        "                         set(np.where((inp == tokenizer.sep_token_id) \n",
        "                            | (inp == 0))[0].tolist()))\n",
        "    #We need to select 15% random tokens from the given list\n",
        "    num_of_token_to_mask = int(len(actual_tokens)*1.0)\n",
        "    token_to_mask = np.random.choice(np.array(actual_tokens), \n",
        "                                     size=num_of_token_to_mask, \n",
        "                                     replace=False).tolist()\n",
        "    #Now we have the indices where we need to mask the tokens\n",
        "    inp[token_to_mask] = tokenizer.mask_token_id\n",
        "    inp_ids.append(inp)\n",
        "    idx += 1\n",
        "#inp_ids = tf.convert_to_tensor(inp_ids)\n",
        "inputs['input_ids'] = np.array(inp_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "d04387b3",
      "metadata": {
        "id": "d04387b3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "3c12e646",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c12e646",
        "outputId": "20af06b6-1ca7-4d74-f219-806c93b82d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 92s 92s/step - loss: 14.1242\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 36s 36s/step - loss: 1.7948\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 34s 34s/step - loss: 1.3070\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 35s 35s/step - loss: 1.7227\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 36s 36s/step - loss: 4.6610\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 36s 36s/step - loss: 1.3311\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 36s 36s/step - loss: 1.3176\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 35s 35s/step - loss: 1.1259\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 34s 34s/step - loss: 1.2770\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 36s 36s/step - loss: 0.9670\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 34s 34s/step - loss: 0.9888\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 41s 41s/step - loss: 1.0397\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 35s 35s/step - loss: 0.8627\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 35s 35s/step - loss: 0.7884\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 36s 36s/step - loss: 0.6436\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 34s 34s/step - loss: 0.5811\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 35s 35s/step - loss: 0.5212\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 38s 38s/step - loss: 0.4524\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 35s 35s/step - loss: 0.3757\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 34s 34s/step - loss: 0.2799\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "history = model.fit([inputs.input_ids,inputs.attention_mask],inputs.labels,verbose=1,batch_size=32,epochs=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3f69ccdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f69ccdd",
        "outputId": "955547c6-885b-433b-8151-88d4cf5bb348"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('kana_to_kanji/tokenizer_config.json',\n",
              " 'kana_to_kanji/special_tokens_map.json',\n",
              " 'kana_to_kanji/vocab.txt',\n",
              " 'kana_to_kanji/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "model.save_pretrained(\"kana_to_kanji\")\n",
        "tokenizer.save_pretrained(\"kana_to_kanji\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b76ee77e",
      "metadata": {
        "id": "b76ee77e"
      },
      "source": [
        "# run it, convert "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "40976b82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40976b82",
        "outputId": "7de2d33b-3acd-4516-e10c-2df67436f5cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at kana_to_kanji.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, TFBertForMaskedLM\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"kana_to_kanji\")\n",
        "model = TFBertForMaskedLM.from_pretrained(\"kana_to_kanji\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a1d6088",
      "metadata": {
        "id": "7a1d6088"
      },
      "source": [
        "sentance = \"自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
        "inp = tokenizer(sentance,return_tensors='np')\n",
        "mask_loc = np.where(inp.input_ids[0] == 103)[0].tolist()\n",
        "out = model(inp).logits[0]\n",
        "predicted_tokens = np.argmax(out[mask_loc],axis=1).tolist()\n",
        "tokenizer.decode(predicted_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "36ac6b1c",
      "metadata": {
        "id": "36ac6b1c"
      },
      "outputs": [],
      "source": [
        "test_split_df['masked_text'] = test_split_df.text + '[SEP]' +   \"[MASK]\"*150\n",
        "test_split_df['unmasked_text']= test_split_df.text + \"[SEP]\" + test_split_df.kana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1b10f9",
      "metadata": {
        "id": "ad1b10f9"
      },
      "outputs": [],
      "source": [
        "score = 0\n",
        "base_score = 0\n",
        "\n",
        "converted_sentance_list = []\n",
        "\n",
        "\n",
        "def jaccard_similarity(sentence1, sentence2):\n",
        "    set1 = set(sentence1)\n",
        "    set2 = set(sentence2)\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    \n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "for index,row in test_split_df.head(5).iterrows():\n",
        "    #sentance = \"自らがオウム真理教ではない別のカルト団体に12年間所属していた経験をもとに [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
        "    \n",
        "    inp = tokenizer(row['masked_text'],return_tensors='np')\n",
        "    inp2 = tokenizer(row['unmasked_text'],return_tensors='np')\n",
        "    sep_loc = np.where(inp2.input_ids[0] == tokenizer.sep_token_id)[0][0] +1\n",
        "    truth_sentance = (inp2.input_ids[0][sep_loc:-1])\n",
        "    truth_sentance = tokenizer.decode(truth_sentance)\n",
        "    original_sentance = (inp2.input_ids[0][1:sep_loc -1])\n",
        "    original_sentance = tokenizer.decode(original_sentance)\n",
        "    print(inp)\n",
        "\n",
        "    mask_loc = np.where(inp.input_ids[0] == tokenizer.mask_token_id)[0].tolist()\n",
        "\n",
        "\n",
        "    out = model(inp).logits[0].numpy()\n",
        "    predicted_tokens = np.argmax(out[mask_loc],axis=1).tolist()\n",
        "    predicted_sentance = tokenizer.decode(predicted_tokens)\n",
        "\n",
        "    print(original_sentance)\n",
        "    print(truth_sentance)\n",
        "    print(predicted_sentance)\n",
        "    base_score+=jaccard_similarity(truth_sentance,original_sentance)\n",
        "    score+=jaccard_similarity(truth_sentance,predicted_sentance)\n",
        "    \n",
        "    \n",
        "print(base_score/test_split_df.shape[0])\n",
        "print(score/test_split_df.shape[0])\n",
        "converted_sentance_list"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}